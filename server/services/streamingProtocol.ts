import { Response } from 'express';

const EXACT_COMPLETE_QUESTIONS = `IS IT INSIGHTFUL? 
DOES IT DEVELOP POINTS? (OR, IF IT IS A SHORT EXCERPT, IS THERE EVIDENCE THAT IT WOULD DEVELOP POINTS IF EXTENDED)? 
IS THE ORGANIZATION MERELY SEQUENTIAL (JUST ONE POINT AFTER ANOTHER, LITTLE OR NO LOGICAL SCAFFOLDING)? OR ARE THE IDEAS ARRANGED, NOT JUST SEQUENTIALLY BUT HIERARCHICALLY? 
IF THE POINTS IT MAKES ARE NOT INSIGHTFUL, DOES IT OPERATE SKILLFULLY WITH CANONS OF LOGIC/REASONING. 
ARE THE POINTS CLICHES? OR ARE THEY "FRESH"? 
DOES IT USE TECHNICAL JARGON TO OBFUSCATE OR TO RENDER MORE PRECISE? 
IS IT ORGANIC? DO POINTS DEVELOP IN AN ORGANIC, NATURAL WAY? DO THEY 'UNFOLD'? OR ARE THEY FORCED AND ARTIFICIAL? 
DOES IT OPEN UP NEW DOMAINS? OR, ON THE CONTRARY, DOES IT SHUT OFF INQUIRY (BY CONDITIONALIZING FURTHER DISCUSSION OF THE MATTERS ON ACCEPTANCE OF ITS INTERNAL AND POSSIBLY VERY FAULTY LOGIC)? 
IS IT ACTUALLY INTELLIGENT OR JUST THE WORK OF SOMEBODY WHO, JUDGING BY THE SUBJECT-MATTER, IS PRESUMED TO BE INTELLIGENT (BUT MAY NOT BE)? 
IS IT REAL OR IS IT PHONY? 
DO THE SENTENCES EXHIBIT COMPLEX AND COHERENT INTERNAL LOGIC? 
IS THE PASSAGE GOVERNED BY A STRONG CONCEPT? OR IS THE ONLY ORGANIZATION DRIVEN PURELY BY EXPOSITORY (AS OPPOSED TO EPISTEMIC) NORMS?
IS THERE SYSTEM-LEVEL CONTROL OVER IDEAS? IN OTHER WORDS, DOES THE AUTHOR SEEM TO RECALL WHAT HE SAID EARLIER AND TO BE IN A POSITION TO INTEGRATE IT INTO POINTS HE HAS MADE SINCE THEN? 
ARE THE POINTS 'REAL'? ARE THEY FRESH? OR IS SOME INSTITUTION OR SOME ACCEPTED VEIN OF PROPAGANDA OR ORTHODOXY JUST USING THE AUTHOR AS A MOUTH PIECE?
IS THE WRITING EVASIVE OR DIRECT? 
ARE THE STATEMENTS AMBIGUOUS? 
DOES THE PROGRESSION OF THE TEXT DEVELOP ACCORDING TO WHO SAID WHAT OR ACCORDING TO WHAT ENTAILS OR CONFIRMS WHAT? 
DOES THE AUTHOR USE OTHER AUTHORS TO DEVELOP HIS IDEAS OR TO CLOAK HIS OWN LACK OF IDEAS?

ADDITIONAL CRITICAL QUESTIONS:
ARE THERE TERMS THAT ARE UNDEFINED BUT SHOULD BE DEFINED, IN THE SENSE THAT, WITHOUT DEFINITIONS, IT IS DIFFICULT OR IMPOSSIBLE TO KNOW WHAT IS BEING SAID OR THEREFORE TO EVALUATE WHAT IS BEING SAID?
ARE THERE "FREE VARIABLES" IN THE TEXT? IE ARE THERE QUALIFICATIONS OR POINTS THAT ARE MADE BUT DO NOT CONNECT TO ANYTHING LATER OR EARLIER?
DO NEW STATEMENTS DEVELOP OUT OF OLD ONES? OR ARE THEY MERELY "ADDED" TO PREVIOUS ONES, WITHOUT IN ANY SENSE BEING GENERATED BY THEM?
DO NEW STATEMENTS CLARIFY OR DO THEY LEAD TO MORE LACK OF CLARITY?
IS THE PASSAGE ACTUALLY (PALPABLY) SMART? OR IS ONLY "PRESUMPTION-SMART"?
IF YOUR JUDGMENT IS THAT IT IS INSIGHTFUL, CAN YOU STATEMENT THAT INSIGHT IN A SINGLE SENTENCE?
HOW WELL DOES IT MAKE ITS CASE?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING IMPOSTOR SCAFFOLDING?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?
IF I WERE TO GIVE A LOW SCORE TO THIS PASSAGE, WOULD I BE PENALIZING ACTUAL INTELLIGENCE OWING TO A LACK OF CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?`;

// Generic LLM caller
async function callLLMProvider(
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  messages: Array<{role: string, content: string}>
): Promise<string> {
  try {
    if (provider === 'openai') {
      const OpenAI = (await import('openai')).default;
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: messages as any,
        temperature: 0.1
      });
      
      return completion.choices?.[0]?.message?.content || '';
    } else if (provider === 'anthropic') {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': process.env.ANTHROPIC_API_KEY!,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: "claude-3-sonnet-20240229",
          max_tokens: 4000,
          temperature: 0.1,
          messages: messages
        })
      });
      
      const data = await response.json();
      return data.content?.[0]?.text || '';
    } else if (provider === 'perplexity') {
      const response = await fetch('https://api.perplexity.ai/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "sonar",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    } else if (provider === 'deepseek') {
      const response = await fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    }
    
    return '';
  } catch (error) {
    console.error(`Error calling ${provider}:`, error);
    return '';
  }
}

// Score extraction function
function extractScore(text: string): number {
  const finalScoreMatch = text.match(/FINAL SCORE:\s*(\d+)\/100/i);
  if (finalScoreMatch) {
    return parseInt(finalScoreMatch[1]);
  }
  
  const explicitMatch = text.match(/(\d+)\/100/g);
  if (explicitMatch && explicitMatch.length > 0) {
    const lastMatch = explicitMatch[explicitMatch.length - 1];
    return parseInt(lastMatch.split('/')[0]);
  }
  
  return 0;
}

export async function executeStreamingComprehensiveProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  res: Response
): Promise<void> {
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // PHASE 1: Answer the 28 questions
  res.write(`üîç PHASE 1: Answering the 28 questions...\n\n`);
  
  const phase1Prompt = `Answer these questions in connection with this text. A score of N/100 (e.g. 73/100) means that (100-N)/100 (e.g. 27/100) outperform the author with respect to the parameter defined by the question. You are not grading; you are answering these questions. You do not use a risk-averse standard; you do not attempt to be diplomatic; you do not attempt to comply with risk-averse, medium-range IQ, academic norms. You do not make assumptions about the level of the paper; it could be a work of the highest excellence and genius, or it could be the work of a moron. If a work is a work of genius, you say that, and you say why; you do not shy away from giving what might conventionally be regarded as excessively "superlative" scores; you give it the score it deserves, not the score that a midwit committee would say it deserves. Think very very very hard about your answers; do not default to cookbook, midwit evaluation protocols.

METAPOINT 1: This is not a grading app. You grade the intelligence of what you are given. If you are given brilliant fragment, you give it a high score. You are not grading essays. You are not looking for completeness (unless the text you are given is clearly such that evaluating intelligence coincides with applying a grading-based metric). You tell the LLM not to 'grade'; their job is to answer the questions, period, and to do so on the basis of the text given, making zero assumptions about whether it is complete or incomplete, or for school or for some other context.

METAPOINT 2: Do not overvalue turns of phrase. An author speaking confidently is not necessarily "shutting down modes of inquiry". In fact, it is likely to be the opposite; by putting a clear stake in the ground, he is probably opening them. Another example: casual speech does not mean disorganized thoughts. Don't judge a book by its cover.

METAPOINT 3: The app should always (in both normal and comprehensive mode) start by summarizing the text and also categorizing it.

METAPOINT 4: The app should not change the grading based on the category of the text: if a text is categorized as 'advanced scholarship', it should still evaluate it with respect to the general population, not with respect only to 'advanced scholarly works.'

METAPOINT 5: This is not a grading app. Do not penalize boldness. Do not take points away for insights that, if correct, stand on their own. Get rid of the idea that "argumentation" is what makes something smart; it isn't. What makes something smart is that it is smart (insightful). Period.

METAPOINT 6: A score of N/100 means that (100 minus N)/100 are smarter (e.g. 83/100 means that 17/100 people in Walmart are running rings around the author).

${questions}

CRITICAL: Do not use markdown formatting. Do not use asterisks, hashtags, or other markup. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).

TEXT:
${text}`;

  const phase1Response = await callLLMProvider(provider, [
    { role: 'user', content: phase1Prompt }
  ]);
  const phase1Score = extractScore(phase1Response);
  
  // Clean up markdown formatting
  const cleanedPhase1 = phase1Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase1Score.toString())
    .trim();
  
  res.write(`‚úÖ PHASE 1 COMPLETE: Score ${phase1Score}/100\n\n`);
  res.write(`üìÑ PHASE 1 ANALYSIS:\n${cleanedPhase1}\n\n`);
  
  let finalScore = phase1Score;
  
  // PHASE 2: Pushback if score < 95
  if (phase1Score < 95) {
    res.write(`üîÑ PHASE 2: Score ${phase1Score} < 95, applying pushback...\n\n`);
    
    const peopleOutperforming = 100 - phase1Score;
    const phase2Prompt = `Your position is that ${peopleOutperforming}/100 outperform the author with respect to the cognitive metric defined by the question: that is your position, am I right? And are you sure about that?

Answer the following questions about the text de novo:

${questions}

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).

TEXT:
${text}`;

    const phase2Response = await callLLMProvider(provider, [
      { role: 'user', content: phase2Prompt }
    ]);
    const phase2Score = extractScore(phase2Response);
    finalScore = phase2Score;
    
    // Clean up markdown formatting
    const cleanedPhase2 = phase2Response
      .replace(/\*{1,3}/g, '')
      .replace(/#{1,6}\s*/g, '')
      .replace(/\_{1,3}/g, '')
      .replace(/\[NUMBER\]/g, phase2Score.toString())
      .trim();
    
    res.write(`‚úÖ PHASE 2 COMPLETE: Revised score ${phase2Score}/100\n\n`);
    res.write(`üìÑ PHASE 2 ANALYSIS:\n${cleanedPhase2}\n\n`);
  } else {
    res.write(`‚è≠Ô∏è PHASE 2: Score ${phase1Score} >= 95, no pushback needed\n\n`);
  }
  
  // PHASE 3: Walmart metric check
  res.write(`üè™ PHASE 3: Walmart metric consistency check...\n\n`);
  
  const peopleOutperforming = 100 - finalScore;
  const phase3Prompt = `You ask the LLM if its numerical scores (N/100, e.g. 99/100, 42/100) are consistent with the fact that those are to be taken to mean that (100-N) people out of 100 outperform the author in the relevant respect. So if a score of ${finalScore}/100 is awarded to a paper, that means that ${peopleOutperforming}/100 people in Walmart are running rings around this person. Is this consistent with your assessment?

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).`;

  const phase3Response = await callLLMProvider(provider, [
    { role: 'user', content: phase3Prompt }
  ]);
  const phase3Score = extractScore(phase3Response);
  if (phase3Score > 0) finalScore = phase3Score;
  
  // Clean up markdown formatting
  const cleanedPhase3 = phase3Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase3Score.toString())
    .trim();
  
  res.write(`‚úÖ PHASE 3 COMPLETE: Walmart-adjusted score ${finalScore}/100\n\n`);
  res.write(`üìÑ PHASE 3 ANALYSIS:\n${cleanedPhase3}\n\n`);
  
  // PHASE 4: Accept and report
  res.write(`‚ú® PHASE 4: Final acceptance and reporting...\n\n`);
  
  const phase4Prompt = `At this point, you accept and report what the LLM says.

CRITICAL: Do not use markdown formatting. Write in plain text only. End with exactly: FINAL SCORE: 85/100 (use actual number, not placeholder).`;

  const phase4Response = await callLLMProvider(provider, [
    { role: 'user', content: phase4Prompt }
  ]);
  const phase4Score = extractScore(phase4Response);
  if (phase4Score > 0) finalScore = phase4Score;
  
  // Clean up markdown formatting
  const cleanedPhase4 = phase4Response
    .replace(/\*{1,3}/g, '')
    .replace(/#{1,6}\s*/g, '')
    .replace(/\_{1,3}/g, '')
    .replace(/\[NUMBER\]/g, phase4Score.toString())
    .trim();
  
  res.write(`üéØ FINAL SCORE: ${finalScore}/100\n\n`);
  res.write(`üìÑ PHASE 4 FINAL REPORT:\n${cleanedPhase4}\n\n`);
  
  res.write(`\nüèÅ 4-PHASE PROTOCOL COMPLETE\n`);
  res.write(`üìä Final Intelligence Score: ${finalScore}/100\n`);
}